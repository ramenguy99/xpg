Python:
[-] Imgui bindings with dear_bindings:
    python .\dear_bindings.py -o pyxpg_imgui_2 ..\imgui\imgui.h --nogeneratedefaultargfunctions
    -> json file looks straight forward for binding gen, nice that we can match API 1 to 1 with what we use in xpg
    -> not yet clear if we can also do ImPlot with this, or if we need internal stuff too. But good starting point.
    -> also not clear what stuff goes to python and what goes to nanonbind autogenned bindings
        -> we definitely want python stubs for autocomplete experience
        -> we probably dont want to do weird cython / ffi stuff in python
        -> we probably want to autogen implementation of the nanobind bindings on the C++ side
        -> we probably want a set of manually generated pythonic helpers for commonly used stuff
    [ ] Implement IM_ASSERT to throw exception? or maybe even just std::abort / std::assert, probably easier
        [ ] Avoid asserts based on missing begin / end pairs, either with context manager or with some other
            workaround to avoid them
[x] gfx and gui bindings for window / vulkan / gui
[ ] Slang bindings for compiling and for reflection
    [x] Compilation
    [ ] Shader cache
        [x] Basic
        [x] Handle multiple files
        [ ] Handle all cache inputs
    [x] Reflection
        - XPG provides only bindings, usage of reflection up to app
        - See at least 2 potential use cases from python:   
            - Auto resolve -> use names in python, descriptor set creation and set/binding/offset computation is automatic
            - Python bindings gen -> offline step builds shaders and creates python bindings to call stuff
    - Slang takes around 200ms to initialize in release and (2.5s in debug)...
    - Include files / deps:
        - Use modules for shareable reusable parts (e.g. helper functions, materials, shadows, etc..)
        - Can use include files in shaders to share common types etc.., less prio than modules but still useful e.g. for cpp interop
[ ] Framegraph implementation on top of reflection for easy prototyping of pipelines/shaders
    -> interesting here would be how much perf we can have when sharing data through arrays, e.g. textures, buffers, uniforms
    -> also interesting if we want to have a full python framegraph or if we can somehow leverage python callables
       + some python bindings to reuse at least the synchronization / traversal logic implemented in C++
[ ] App ideas:
    [ ] Expose compute and raytracing -> redo raytrace
    [ ] Try to redo sequence, can i read directly to bufs without gil?
    [ ] Huge voxel engine
        -> brickmap
        -> packed coords in storage buffers
        -> auto cube trick
        -> gpu frustum culling
        -> SVO traversal?
[x] Use within conda:
        conda install conda-forge::vulkan-tools
[ ] Copy / upload / blit utils

Fetaures:
[ ] Better feature / profile management:
    [x] Optional debug_log / validation / gpu based validation / synchronization validation
    [ ] Headless mode
    [ ] Preferred features with a way to fallback
    [ ] SYNCHRONIZATION_2 is not actually that widespread, maybe find a way to
        ship the layer or maybe port the logic from the extension layer in case
        it's not supported?
[ ] Object names / debug API (names should be optional on python side, otherwise annoying to specify them during prototyping)
[ ] Add helpers for timestamp queries
[ ] Upload helper for buffer that works if device memory cannot be mapped
    [ ] Also batch uplod helper for small images / buffer instead of copy + sync for each
[ ] File formats:
    [ ] Do we need this in python? Or do we keep it there?
[ ] Meshlet stuff:
    [ ] Integrate meshoptimizer
    [ ] Rasterize meshlets
        [ ] Vertex shader
        [ ] Mesh shaders
    [ ] Raytrace CLAS
    [ ] Python bindings

Build:
[ ] cmake install double check
    - Avoid add subdirectory shit
    - GLFW now is a public dep because we statically link with it and use header files in public headers
        -> Header files can be made private by copying out constants / forward declare needed stuff
        -> can it be statically linked privately somehow? e.g. objects target? include source?
[ ] LZ4 is dynamic on windows, no clue why
[ ] python builds
    [x] pyproject.toml
    [x] test wheel
        pip wheel . --verbose
    [ ] sdist:
        [x] make sdist
            python -m build --sdist --verbose
        [ ] build from sdist (???)
    [ ] cibuildhweel
        [x] Local
            - create venv
            - 
        [x] manylinux
        [x] musllinux
        [ ] windows
        [ ] mac
        [ ] conda build, likely require xcb dependency (similar to vulkan issue?)

Run python examples:
cmd /C "set PYTHONPATH=install\python && python python\example.py"

Swapchain:
[ ] Try resize without waitidle, just recreate swapchain, start using new swapchain, delete old when no more frame is using it.
    -> not entirely clear what happens when you have 3 frames in flight, can you potentially have 3 swapchains alive
       at the same time? Or do you end up calling wait idle at that point?
    -> See: https://www.youtube.com/watch?v=mvkHYAu7i6c
    -> See todo in gfx.h, needs helper on application side, otherwise very complex to do right, should probably sacrifice
       perf during resize (e.g. cap to 2 swapchains in flight) instead of having growing history.

Descriptors:
- To run a pipeline need to bind the related descriptor sets
- Those can be known statically or learned through reflections
- No need to rebind across pipelines if these can be shared
- Need to do this in increasing frequency, because after binding something incompatible the later descriptors are disturbed

Framegraph:
- The 3 main advantages are:
    - automatic barriers on resources
    - memory aliasing
    - ideally library of reusable components -> would be very nice, but unknown how easy to get the ergonomics right for this
- Not obvious how to handle descriptors in this case:
    - Gray: the user does not need to care
        - descriptors for owned resources are allocated by the render pass
        - descriptors for external resources are provided on render
        - granularity is per descriptor, which means we HAVE to do copies CPU -> GPU or GPU -> GPU on draw
        - can we somehow extend this to work per set?
        - binding frequency and granularity does not necessarily match ownership
    - User in control:
        - Pipelines have a well defined layout. At creation time we know what descriptors we can generate or validate this layout.
        - Descriptors are really just part of the signature of a render pass. Together with other parameters that might be required.
        [ ] Think about an API that allows us to define either statically or dynamically the descriptor sets that should be passed when invoking this pass.
        - The global constants problem:
            It is very likely that you have some scene constants that are the same for all shaders in an app.
            On descriptor set 0 you might have (among other things) a constant buffer for those constants.
            On shader side those are reusing the same struct -> always in sync.
            On c++ side we can do the same thing, just share the struct. (hopefully can make this work with slang somehow)
            On binding side this descriptor set can be the same for all compatible pipelines.
- Static vs dynamic graph:
    - Graph built per frame. Can specify all needed params every time -> requires caching, at least for owned resources.
    - Graph built per at startup and then run.
        [ ] How does the user pass dynamic parameters to the graph?
            -> Keep handles (either the task object itself or some returned reference)?
            -> Everything through function pointers? Seems messy and hard to get right, control flow will be very weird, almost impossible to do threading then.
            -> Try to design some tasks and see how those could look like: (simple: copy, full screen pass. more comples: ping pong blur, gen mips, draw opaque, gbuffer, shadows).
- Another API would be to do state tracking on a resource (likely manually or through helpers for each usage on binding) and then deduce barriers:
    - maybe more flexible when prototyping?
    - more similar to d3d11 / opengl approach
    - less efficient potentially? Extra work done for you can maybe be opt in?
    - Is specifying barriers really that annoying if we have sync helpers (to validate with vulkan sync helper lib -> this looks similar to what we were doing and could be a nice set of helpers / reference)
- Thoughts:
    In favor of graphs:
        - Composability of self-contained parts (if possible to achieve in a sane way)
    Against graphs:
        - I don't think the flamegraph is the correct unit of granularity for simple programs / prototyping -> too retained:
            - hard to share descriptor layouts
            - can't control order of execution of passes -> how to know what needs to be rebound -> maybe need to always rebind from scratch or do complex state management.
            - hard to do dependency injection in the meat of the passes
        - Framegraph can be built on top of this API if you need it
        - Resource creation can be done manually at init time
        - API MUST be immediate mode when calling render passes
        - We can have synchronization helpers on resources instead of frame graphs.
        - Reusable components can be more flexible instead of requiring retained mode processing.
            -> sketch API of how this could look like
    [ ] Try to rewrite our exmaples and a viewer-like app with both styles, see what looks better.
        -> can try to do something a vulkan backend for aitviewer
        -> also want to support more advanced use cases like indirect rendering, batched meshes, per frame upload with triple buffering, gpu driven stuff, raytracing, etc..

Backlog:
[x] Rename lru cache
[x] Cleanup project structure:
    [x] Fix build and cleanup all of them
    [x] One directory per sample app
    [x] Normal build instead of unity?
[ ] minimal app (multiple hooks seem to be needed, e.g. pre-gui, post-gui at least, maybe should just be a single loop one? but then more dup?)

Ideas:
- Maybe have an internal feature set kind of thing to support multiple hardware types.
  Seem important to support at least:
    - Compute only (required: subgroup stuff?)
    - Basic rendering (required: dynamic rendering?)
    - Raytracing (required: rt + bindless)

- New error model for everything:
    - Error on the handle kind of thing
    - Ability to attach context and record call stacks
    - Maybe helpers / macros to check if an handle is errored and collect callstacks / attach additional info.

- Cleaness and portability:
    - Statically link on windows and with musl on linux (no idea about macOS)
    - Get rid of some headers
    - Maybe split stuff in some cpp files

Notes:
- Add python library target and test nanobind + pybind11 stubs generator for generating and packaging python module
    - Python module interface (similar to viewer needs):
        - Window utilities (multiple windows, surface rendering without window / swapchains)
        - Rendering logic and scene hierarchy/renderables likely defined in C++ code and exposed to python. (Maybe add wrappers for conversions/utils that are easier to do in python).
        - Debugging options at runtime (e.g. logging / tracing / validation / named objects etc..)
- Targets (share as much code as possible):
    - Standalone apps in C++.
    - Scriptable apps from python.
    - Headless rendering/compute from C++/python.
- Initialize vulkan stuff and build rendering layer utilities on top of it.
    - Required extensions?
    - ImGui/docking support -> use same GUI version as pyimgui and share ctx somehow, or make our own bindings.
    - Render on resize mode?
    - Binding helpers?
    - Synchronization / render passes?
    - Resource / memory management (VMA)?
    - Ring/buffers?
    - Ray-tracing?
- Shaders:
    - HLSL/GLSL/SLANG? -> all require toolchain to be installed for development
    - Need introspection?
    - Hot reloading?
- Test on linux/macos.


Project structure:
- Core:
    - Vulkan context and headless rendering
    - Resource creation and other utils
    - Shader and pipeline management helpers
    - Maybe a rendergraph helper here too?
- Viewer:
    - Windowing
    - ImGui
    - Input
    - Renderables and passes

- Python module:
    - Bindings for both core and viewer

 Core    -
  |        \
  |          -> Python bindings + conversion utils -> python wrappers/helpers -> user code
  V        /
Viewer   -




Initializers notes:

Likely ok to have an extra read-only span for this, ArrayView
is inherently read-write.
We can also delete copy constructor stuff ehere that allows
it to be even more typechecked (cannot create with initializer list and then assign when list is destroyed)
struct A {
    int a;
};

This is would also be nice, but unfortunately the c++ compiler cant reason about it when
it's inside a struct, only as top level param. Guess we gotta dynamically allocate.
template<typename T, size_t N>
using SpanFixed = T[N];

template<typename T>
struct Span {
    const T* data;
    size_t length;

    Span(std::initializer_list<T> l) {
        data = l.begin();
        length = l.size();
    }

    Span(const Span& other) = delete;
    Span& operator=(const Span& other) = delete;
};

struct B {
    A a;
    Span<int> span;
};

void construct_safe(const B&& b) {
}
B b = {
    .a = {.a = 5},
    .span = { 1, 2, 3 },
};

Span s = { 1, 2, 3 };
construct_safe({
    .a = {.a = 5},
    .span = { 1, 2, 3},
});

Span span = { 1, 2, 3 };
construct_safe({
    .a = {.a = 5},
    .span = std::move(span),
});

Perf notes:
- Index buffer was on GPU -> double check wtf logic is there in VMA to cause this (likely because we want it host visible, but we are preferring device local?)
    - this was messing heavily with PCIe perf and slowing draw significantly confusing the trace
- on my RTX3060 mobile copy queue seems to be way slower than async compute queue (and also slower than BAR writes)
    - copy     -> 16%
    - compute  -> 22%
    - PCIe BAR -> 38%
- when not using nsight actually I get the same performance with async copy compute queue and PCIe BAR writes -> likely closer bandwidth
- All in all we have 2.5ms draw and ~3.5ms transfer. We are just transfer bound, but with the copy queue we keep the CPU free during those transfers.
[ ] Try with buffered stream loading directly to upload heap -> slightly more complex because we have more buffers then frames there
- https://ctf.re/about/