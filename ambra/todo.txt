Priority:
- [x] Lights, materials and shader permutations
    - [x] Cache shader compilations also in memory
    - [x] Ambient light
    - [x] PBR material
    - [x] IBL
        - [x] Cubemaps
        - [x] Sampling
    - [x] Try loading PBR model (e.g. helmet)
        - [x] sRGB textures -> already supported by specifying format on image property?
        - [x] normal mapping
        - [x] Mips -> Integrate FFX?
            - [x] Can't really explain the SPD_MAX_LEVELS + 1 yet, maybe the max does not include level 0?
                It in fact does not. This means we can downsample up to 4096x4096, which should be fine.
                The SPD kernel assumes that after the first round of 6 mips only up to 64x64 is left,
                and only 1 thread group remains for the last 6. I you bind a bigger texture the layers after
                the 6th are cropped as if layer 6 was in fact up to 64x64. It does not seem impossible to me
                to generalize the code to let more thread groups survive after the first round. Another option
                sould be to blit or invoke the kernel again on just the first few layers, the dispatch overhead
                shouldn't be that bad for > 4096x4096 texture anyways.
            - [x] Figure out if MUTABLE_FORMAT is fine for sRGB, only other option is to copy after downsampling, seems to be fine.
            - [x] Fix barriers for mips, enhance or move away from from_data -> changed from_data to transition the whole resource.
                The pyxpg way of storing the layout per image only works if all mips have the same layout. We don't yet have a use
                case for having separate layouts so we don't expose this yet.
        - [x] Mip generation for materials
            - [x] Start simple, could potentially batch those generations at some point.
    - [x] Expose sampling options in materials
    - [ ] Adding / removing and enabling / disabling lights is still a bit tricky.
    - [ ] Point and spot lights and shadows
- [x] Shaders / fonts build and packaging into wheel
    - [x] Explore using hatchlings instead of setuptools for better editable builds (read pep 517)
- [x] Important 3D primitives
    - [x] Grid floor
- [ ] Test edit API and delete
- [ ] Rethink Animation and how it relates to:
    - [ ] property edits (currently separate)
    - [ ] first/last frame and boundary
    - [ ] Maybe implement DB browser to get more context
- [ ] Cache pipelines -> requires hashable pyxpg objects
- [ ] Stages presets in config or in helper functions (empty, origin, floor, lights, maybe more?)
- [ ] Basic client/server example and initial set of APIs
- [ ] Multi-viewport
    - [x] UI:
        - [x] Reusing surface requires injecting between imgui commands and stopping / resuming render pass
              -> we can probably afford the extra blits in most cases
              -> injecting imgui also has issues with viewport
        - [x] Clipping the viewport to the window could help with making rendering cheaper.
              -> check if the API exposes this in some way or if we need to change the projection for this to work
              -> anyway probably have to adjust projection, but will also need to add the concept of an offset, not just size aspect ratio
        - [x] The root node in ImGui seems to be special, in the sense that it does not resize like their children do when children take portions of its space.
              The API could be something like:
                - Opt in to using viewport system
                - By default you get a single viewport window already docked to root node.
                - If you undock and resize this you see the clear color in the background, but nobody is drawing there.
                - Gui impl:
                    - Create a window per viewport with default layout that just tiles within main window.
                    - During GUI update get content sizes and ensure we have an image for each viewport to render into,
                      write it to a descriptor set and submit it as full-window image for that viewport
                      -> need to be careful with synchronization because we might need to resize viewports while they are being
                         drawn in the previous frame. We either need to stall on resize (likely not gonna be super smooth) or
                      -> we could have a resize backoff logic where instead of continuously destroying /  updating on resize
                         or even alloc always the window size and just render/sample a subset -> sampling should always be exact anyway
                    - During rendering render to each of this image then transition to shader read optimal before rendering GUI.
                    - If you have multisampling on you actually need a multisample buffer for each viewport too
                        -> actually since we render in sequence you could always reuse the same one
        - [ ] Would be nice to highlight viewport borders when active (also maybe on hover with slightly dimmer tone)
    - [ ] Viewer:
        - [x] View dependent things need to be recomputed and potentially require multi-buffering:
            - need to sort gaussians for both views
                - need to have CPU and GPU resources per viewport
                - or need to pre-render for 0 -> render for 0 -> render for 1 -> prerender for 1, but still need to buffer CPU resources
            - no-need to re-render shadow maps
        - [x] Need to deduplicate uploads
            - properties and gpu properties are sets -> should already work
            - objects currently are lists(assuming the same object never appears twice in the scene hierarchy)
            - Viewports could default to same scene
            - If each viewport has a scene an object might have a parent in one scene but not in another.
                - Maybe a viewport mask type thing is easier than multiple scenes
                    -> objects have a viewport mask, if None render to all viewports (default)
                    -> Move scene outside viewport (also easier to viewer.scene)
        - [ ] Think about the API and UI for enabling / customizing this
            - [ ] Shortcut to split
            - [ ] viewports becomes a list, api to create a new viewport according to config or from scratch
            - [ ] scene anyway moves before
            - [ ] think about the camera, especially with respect to having cameras in scene
        - [ ] Missing:
            - [x] Camera mouse interactions with different viewport
            - [ ] Object viewport masks
                - [ ] Mask is optional int, None means all
                - [ ] Lights with masking -> plan is to make this more immediate mode, upload lights every frame and only if enabled and the mask matches the viewport
            - [ ] Maybe move some viewport code inside renderer? It seems like now it's quite renderer specific, but currently lives in viewer.py
            - [ ] GaussianSplatting is not yet ready for multi-viewport (need to split all uploads and descriptor writes into "upload", now interleaved, including for sorting).
- [ ] Windowless viewer
    - [x] Screenshots
    - [ ] Video, maybe using av? Maybe just user callback? Make PoC then decide
        - [x] PoC with PyAV
        - [x] Do pipelinening and check speedup
        - [ ] Frame / timestamp range customization with different playback and render rate
- [ ] Colormap material with user-configurable min-max and colormap (evaluated in shader from R channel)
    - think about how to also maybe support variants like versions dependent on distance to position / plane
- [x] Remove all the get_current() that we don't need from render() (to avoid spurious streaming loads)
- [ ] Fix leak issue with non ref-counted objects as default argument. Maybe open issue in nanobind to check if intended.

TODO:
- [ ] Primitives
    - [ ] Objects:
        - [ ] thin points 2D
        - [x] thin points 3D
        - [x] thin lines 2D / 3D
        - [ ] circles and spheres
        - [ ] Cilinders (with/without caps)
        - [-] Meshes
            - [ ] Handle optional params for:
                - normals -> fallback to flat shading with shader derivatives
                - uvs -> fallback to flat color
                - tangents and normal maps (?)
                - joint indices, weights, joints
            - [ ] Fixup when correctly handling materials and
        - [-] Grid floor
            - [ ] Height control
        - [x] 2D Images
        - [x] 3D Images (e.g. textured quads)
        - [ ] There are multiple types of simple shapes you could want:
            - [ ] Instanced objects with normal materials -> basically the current mesh we have now + instancing (could be added to mesh, could be separate)
            - [ ] Instanced with single fixed color for all objects -> similar to diffuse material but color comes from uniforms (e.g. push constants)
            - [ ] Instanced with different color per object -> similar to diffuse material but with color coming from buffers + instance_id instead of from uniform / (sampler + uv)
            - [ ] The 2 above could also come in a variation where you have more than one color per object (but a bounded amount, e.g. 4)
                - Can implement this by having a separate object for each, but then you need to 4x a lot of stuff which does not help scaling.
                - One option is to do tint and have different shader permutation that tint
                    -> need to remember to create a white material for this (could have logic that defaults to diffuse (1, 1, 1, 1) instead of (0.5, 0.5, 0.5, 1.0) if vertex colors or intance colors are passed in)
    - [ ] Add helpers to simplify creation of vertex buffers and descriptor sets
    - [ ] Together with light / materials, figure out how to do shader permutations / optional properties
- [ ] Renderer architecture / path tracer support
    - [ ] Single object vs batching -> especially relevant for path tracing where you need to batch things in a tlas
        - [ ] Lights
        - [ ] Materials
        - [ ] Instancing
        - [ ] Support both raster and path tracing with accumulation
        - Need to think what level of control the renderables need over how they are rendered.
          Things like transparency and shadow map rendering might want to have something like this anyways.
    - [ ] Immediate mode 3D api for things like I am hovering on this pixel / object and I want to see an arrow / sphere / lines
    - [ ] Auxiliary buffers for picking and inspection:
        - [ ] gl_PrimitiveID is expensive on modern hardware when using the traditional pipeline.
              It can be cheaper with a mesh-shader based pipeline bat MoltenVK does not support that atm.
        - [ ] it could potentially be possible to have an option for it but that also increases number
              of shader permutations.
        - [ ] also not sure if we need to always render this or if we can only render it on-demand, depending on UI
- [ ] Descriptors
    - [x] See what patterns we can find in primitives and if we can reduce amount of scene / material / object bindings by using compatible pipeline layouts.
          Custom renderables might still want to have full control of bindings, but we can't interleave draws that want to reuse existing bound sets,
          in that case we would need to render those at the end? Can be somehow similar to material based sorting, if it becomes a thing.
    - [ ] Check if use of descriptor update templates can be useful to unify and speed up descriptor writes
- [ ] Buffer and image management:
    - [ ] UploadableBuffer and UploadableImage are currently only used
          by renderer for bulk uniforms. I think the main issue with this
          is that it only solves the startup / first time upload issue, which
          can already be solved by .from_data.
          It does handle DEVICE_MAPPED_WITH_FALLBACK but .from_data also does.
          The fact that it keeps the staging buffer around is not that useful because
          we can re-utilize it during rendering only if we alloc 1 per frame.
          Shapes of upload:
          - Sync upload once at creation                               -> .from_data
          - Sync upload once after creation (batched)                  -> bulk_upload
          - Per frame upload with one GPU buffer per sequence frame    -> preupload GpuProperty (lazily allocates staging buffers, shared across frames)
          - Per frame upload with single GPU buffer for whole sequence -> streaming GpuProperty (preallocated staging buffers, shared across frames, also supports prefetching with additional staging and GPU buffers)
          We are technically missing but we might not need it:
          - Per frame upload without a sequence (e.g. bulk uniforms) -> this is in spirit the same as streaming GpuProperty but without relying on it (maybe GpuProperty should rely on this?)
            - For GPU resources (non-mappable buffers or images)
            - For buffers we should check if BAR memory is available and if we want to use and alloc per-frame mappable buffers, otherwise alloc per-frame staging and 1 device resource.
    - [ ] Prefetching is currently assuming linearly increasing frame indices
    - [x] Together with server, figure out API to append frames (and potentially insert / remove too)
    - [x] Support buffer suballocation in gpu property for preuplaod and image array to reduce number of resources that we need for a sequence -> could require changes at usage side that will also get an offset
    - [x] Implement invalidation of preuploaded properties. If using mapped buffers (CPU or BAR) we can just alloc the remaining frames_in_flight - 1 buffers and switch to ring buffer.
    - [x] Implement smarter logic to pick which upload mode to use. For smaller properties we want bar if available I guess.
- [ ] Server
    - [ ] Test design and finish implementing
    - [ ] Client python package (maybe shared ambra-protocol too, could require same version until 1.0.0 and then keep server backwards compatible within major)
    - [ ] C/C++ and rust lib
- [x] Frame helpers (ideally at object level, shared across all primitives)
    - Fixed framerate
    - Variable rate
    - Missing frames / holes in data (different from hold last pose)
    - Repeat vs hold vs disappear at end of sequence
    -> think about how this fits together with streaming/prefetching/animated properties
    -> also think about how to visualize this in a timeline viewer
- [x] Transform properties, kinematic trees and node descriptor for constants
    - [x] Transform 2D
    - [x] Cleanup constants dup
- [x] Uniform pool helper
    - [x] Rework how we do descriptors for this. It seems reasonable to me that we will keep the pool
          for memory managemnt / suballocation, but we need to have custom descriptors written every frame.
          Anyways most primitives will have a set per object, right now plan is to have the object contants to
          be the first binding of the object set.
- [x] Images and upload
- [x] .create currently has to be called manually on primitives
- [x] Streaming properties
    - [x] lines with list instead of np array
    - [x] heterogeneous size with max_size
    - [x] streaming prop
    - [x] trigger all uploads before blocking for rendering -> made properties managed by renderer
    - [x] prefetching
            -> likely make a simple mesh class and recreate sequence
            -> this means also porting some GUI, likey making some helpers, ideally also adding the profiler
    - [x] unify GpuBufferProperty and GpuImageProperty
    - [x] Think about getter/setters to update properties and explicit redraw to update properties
    - [ ] More viewer playback controls (e.g. timeline view, property inspectors)
        - Look at imguizmo sequencer, but likely re-implement with custom use-case in mind
        - Likely want to do most of the ui work in C++ and just offer callbacks for input like click/over/tooltip etc..
        - Look at rerun timeline for feature ideas
- [x] Camera movement
    - [x] Define interface / config / keybindings
    - [x] Initialize camera controls with up / distance from config
    - [x] Implement getters for front, up, right vectors from camera
    - [x] Drag start / end detection
    - [x] Camera mode dependent rotate/pan
    - [x] Scroll zoom (how to handle moving the target (or reducing the distance)?)
    Extra:
    - [ ] 2D controls -> pan only and change zoom mode to modify ortho size?
- [ ] Viewport:
    - [x] resize
    - [ ] multiple viewports
    - [ ] related to UI if decide to do viewports in ImGui windows, not clear how to do default placement (check docs)
- [ ] UI: -> likely in common ui place that can be customized / modified (helpers for things like default layout as well, likely configurable)
    - [x] Scene tree and property view -> custom widget callback per property?
    - [x] Playback UI
    - [x] Fps display for debug
    - [x] Configurable keybindings
    - [x] NEXT: Expose imgui.get_io for things like want_capture_mouse (or add helper for that)
    - [x] Show memory usage in stats
    - [x] Better/configurable handling of imgui.ini file
    - [x] User texture for UI stuff, e.g. in imagui window (or tooltip)
        - ImGui uses VkDescriptorSet, must be a single COMBINED_IMAGE_SAMPLER in it
        - You can pass the descriptor pool from which to allocate to imgui directly, but we currently
        let ImGui do that and just pass a number
        - You can allocate from ImGui's pool with ImGuiImplVulkan_AddTexture and ImGuiImplVulkan_RemoveTexture,
        these will allocate from the imgui's managed pool. This only requires a VkSampler, an image view and a layout.
        - We can also allocate those descriptor sets manually, either directly in python or in a Gui wrapper.
        - It might be a bit inefficient to have a DescriptorSet and Pool per image, but we currently don't expose
        descriptor pools anyway. We might want to revisit this once we do this.
    - [ ] Port built-in profiler
    - [ ] Memory breakdown view -> also plot of usage per-frame to spot mid-frame allocs
- [ ] Shaders
    - [x] check if there is a way for users that are writing custom shaders to import ambra modules (like scene.slang)
    - [ ] cache + hot reloading for dev (?)
    - [ ] export to spirv during package
    - [ ] run spirv-optimizer (for sure on export, but maybe even during slang build)
    - [ ] do we want a pipeline / layout cache? Can we do something using frozen dataclasses as hash keys?
- [x] Config:
    - [x] Improve camera initialization (especially ortho for 2D case)
    - [x] vulkan validation options
    - [x] add a way to set viewer position in addition to size? (expose glfw for this)
    - [x] Think about clean way to handle handedness / zy up conventions.
        - [x] wrap things that care in a separate module, make different default symbols base on config ?
        - [x] maybe math and camera utils should expose both, and viewer use config to pick
        - [ ] currently configurable but not used everywhere, double check that it is propagated where needed
    - [ ] load from disk / yaml + default locations (e.g. home? cwd?)
- [x] Hook pyxpg logging into python logging
- [x] separate concepts of frames in flight and number of swapchain images -> needs xpg changes too
- [ ] Cleanup before release:
    - [ ] Public vs private API (hide / protect internal stuff)
    - [ ] Unit tests on all python versions (likely with lavapipe on CI)
    - [x] Rename some badly named things:
        [x] UploadMethod constants
    - [x] Is StreamingProperty any useful? Should users instead just subclass Property? Removed
    - [ ] Re-organize utils directory, what really is a util anyway?
- [ ] Extra Features (likely at viewer level with pyxpg helpers / wrappers (optional xpg features enabled on python release)):
    - [ ] Meshoptimizer + meshlets
    - [ ] Gaussian splats
        - [ ] Move into shaders/3d, it's integrated enough that we kinda need that.
    - [ ] Ray marching / octrees
    - [ ] Marching cubes
    - [ ] Pointclouds
    - [ ] Framegraph
- [ ] Customization workflow
    - [x] Completely new shader
    - [x] Shader that reuses common scene stuff or utils
        - [x] Import form ambra shader directory
        - [ ] Reuse materials code but (with customizable descriptor layout ?)
        - [ ] Customize material / shader for existing primitive (maybe just copy -> edit workflow?)

Bugs:
- [x] Fix non-unit world_up results in nans in camera
- [ ] Why does @cache on shader break refcounting on context, especially for an object that does not have any reference to ctx
      -> investigate further and maybe make minimal repro if it's actually a nanobind issue
- [ ] Exceptions sometimes cause vulkan validation errors on wrong frees

Issues found while implementing skeletal animation:
[ ] if passing [N, 1] instead of [N] we can end up creating LOTS of frames (and therefore LOTS of buffers)
    -> we can likely check if last dimension is 1 in something that usually is not (e.g. indices) and warn / raise
    -> We can anyways try to optimize number of buffers that we actually create with suballocation
[ ] easy to pass an image as np.float64 if you are doing ops (e.g. dstacking alpha), warn / raise if dtype does not match format

Benchmarks:
- [ ] Big sequence
- [ ] Big frames (e.g. DM dataset)
- [ ] Lots of objects -> create synthetic benchmark with variations in property types

Tools:
- [x] mypy:
    # configured in tool.mypy
    pip install mypy
    mypy ambra
- [x] ruff:
    # configured in tool.ruff.*
    pip install ruff
    # Sort imports
    ruff check --select I --fix
    # Format
    ruff format
    # Lint (not including examples)
    ruff check --exclude examples
    ruff check --exclude examples --fix
- [ ] clang-format shaders
- [ ] run on CI once we have automated build / tests


Notes:

Live streaming implementation notes:
- ListBufferProperty + UploadSettings(batched=False or streaming=True) are needed to avoid awful perf
- Editing frame in on_gui (e.g. clear button) is trick because scene.update() already happened. Maybe we could have a before_update gui
  and an after update one. Or also just part of the API that you need to re-update properties.

DB viewer implementation notes:
- Need prefetch to work also
- If you use get_current() on a property in renderable (for example to get the number of points to draw) you trigger a
  get_frame_by_index per frame, which in the streaming case is bad because this is not cached.
- If we cache it on update we have the weird behavior that now on update you always get a load that does not now about the GpuProperty cache.
  -> if we were to redesign from scratch we maybe separate the concept of CPU and GPU caching a bit more instead of having both happening inside the GPU property.
     this might be easier said than done since the GpuProperty is actually doing CPU memory management as well right now because it wants to
     fill GPU visiblee CPU buffers directly.
  -> for now we can avoid this by not calling get_current on render, but this is potentially easy to do by mistake
  -> this also interacts weirdly with async_load because if you call get_current on the CPU you also need to handle main thread loads (thread_idx = -1), which
     if used naively to index resources would result in concurrent usage of the last item in the resource list.

New property API plans:
- Deps:
    - [x] New synchronization API:
        - [x] Remove memory usage concept and add back stages and access -> also removes API surface and allows more combining -> in the end decided to keep other API because tremendously less verbose and still useful
        - [x] Make access default to READ | WRITE
        - [x] Expose combined barriers API (list of memory, buffer and image barriers)
        - [x] Also expose image barriers with prev layout
        - [x] Only limitation left is that we cannot do different levels with different layouts (which vulkan would support) -> we can the tracked layout just goes stale, but you can fix it manually if you know what you are doing
        - [x] Make sure we use HOST_READ bit for readbacks (flushes GPU caches after writes to host memory). We don't
              need this for uploads because queue submission also implicitly does that.
- Plan:
    - [x] Split DataBufferProperty into ArrayBufferProperty and ListBufferProperty
        - [x] Remove usages of as_buffer_property from examples
        - [x] Test that this actually works
    - [x] Can now use constraint_dtype=None to let as_buffer_property infer it for us -> test this for SH dtype
    - [x] Re-organize scene traversals / collection (ideally minimize)
            - Scene traversal notes:
                - Startup:
                    One full traversal on first frame to collect max_animation_time
                - Loop:
                    - Call update on each unique property
                    - Call update_transform on each object -> also disabled
                    - Renderer:
                        - Call create_if_needed on each enabled object
                        - Call create_if_needed on each unique enabled property
                        - Call load on each enabled property
                        - Call upload on each enabled property
    - [x] Move GPU property inside property -> maybe also take advantage of split between List and Array for is_jagged stuff in GpuBufferProperty (or add new API with default that exposes this info)
        - [x] Add the concept of a gpu usage (to collect constraints)
        - [x] Reroute gpu property creation and load/upload stuff through properties
        - [x] Enable mip generation and sRGB
            - [x] Include extra data in GpuResourceProperty -> e.g. offset + size for buffer, ImageViews for images
            - [x] We now have the concept of a default image view that covers the whole image. This is mostly useful to create descriptors from an image and to render to an image.
                  Maybe we should again move away from this abstraction and have users create their views independently. For example this view is not useful for texture arrays
                  used for batching where you want to view a slice at a time -> now keeping this for convenience and creating additional views for mip generation when needed.
            - [x] sRGB does not seem to be handled properly by SPD. As far as I can see SPD always converts from linear to sRGB and does averaging in sRGB.
                  My understanding is that we would want the opposite of this. Sample does sRGB -> linear (either automatically with sRGB view or manually when reading
                  from storage image, this includes the mid mip) and we linear -> sRGB manually before storing.
    - [x] New sync API
        - [x] Combine pipeline stage and access flags
        - [x] Batch upload and mip generation barriers
                -> Need to store/exec all image barriers that require layout transitions
                -> Need to store/exec all queue ownership barriers
                -> Need to store/exec all memory barriers (could potentially remove duplicates / combine)
        - [x] Image layout breaks down when you start having texture arrays or mips with different elements in differents layouts
                -> for batching images into single texture array we likely want to lift this constraint (not so important at creation, but for e.g. updating a single frame)
        - [x] Cleanup and batch more barriers in renderer
    - [x] Streaming mip creation and barriers -> rethink together with batches synchronization
    - [x] update, append, insert, remove frames -> I am ok if batching is only allowed on Array* variants and those do not allow remove/insert (but do allow update).
        - [x] Both API objects and underlying data needs to be updated
            - [x] API objects are likely in list and can be cheap to move / realloc
        - [x] Texture arrays and batched buffers are implicitly incompatible with append / insert / remove
              - Default to recreate gpu property if this happens.
        - Notes:
            - [x] It makes sense to me that some operations just cause the
                  GpuProperty to be destroyed and recreated next frame.
                  We could mark with flags on the property what operations
                  are supported and recreate for the others, this means we need
                  to check for all properties if they should create their gpu
                  property every frame though, now they assume the have been
                  created already.
                  -> now objects are created in renderer (potentially bad)
            - [x] Not clear what is the best way to destroy existing properties
                  considering that they might be in use in the current frame and
                  we currently do not have a reference to the renderer to allow
                  edits to happen anywhere.
                  We also probably don't want to do any GPU operations here
                  because we might actually be on a different thread when this
                  happens. Need to think about race conditions here really,
                  maybe this is just not supported and that's it.
                  But anyways we might not have access to the renderer in every
                  case (unless we store it in the property).
    - [x] Buffer suballocation / texture array for huge number of frames:
        -> can't afford one buffer per-frame when uploading lots of frames
        -> currently can work around this with streaming API
        -> texture arrays have actually a max 2048 frame max, I don't like this limitation
           so I think we can give up on the idea and just have batching for buffers.
    - [x] ideally remove or integrate generation index, I'd like it if we
          did not have stuff that scales in cost or memory with number
          of frames for the streaming case, it's also one more thing to keep in
          sync when updating / removing frames, but maybe unavoidable?
        - [x] LRU Pool is very overloaded right now, I am not sure if we actually need it
              or if we can simplify stuff (maybe take out in flight stuff and generation index)
              -> weird that we use in flight of size 1 for the GpuPool
              -> weird to give all generation index stuff to LRUPool, but in some way makes sense because the LRU
                 pool can evict based on generation information
              -> maybe instead of removing the whole generation concept make it completely internal instead and
                 keep track of generation indices only for elements in the LRU pool (instead of everything)
                 (maybe even switch around and keep 0 for latest and increment for eviction)
              -> Finally decided to keep it and integrate the generation concept in it but without allocating
                 more than the number of buffers in the pool.  Still needs some testing.
              -> While it's true that it's become big we need all of this functionality for the streaming property,
                 we could split it in more parts later if needed, but now the only user really uses all of it.
    - Resource usecases:
        Preuploaded:
        - Uniform / varying preallocated array (Array*Property for uniform, ListProperty for varying, GpuProperty with batched alloc, invalidate for reupload)
        Async with accumulation:
        - Sensor / streaming data, accumulated but untouched -> data generated on a separate thread at variable rate (List*Property + GpuProperty with streaming alloc)
        Async with replacement (same as preuploaded dynamic content, even though data comes from a different thread):
        - Sensor / streaming data always replacing previous element (ArrayProperty + Gpu property with batched alloc, invalidate for reupload)
        Async with ring-buffer style accumulation:
        - Sensor / streaming data, accumulated in ring-buffer but untouched (not supported by default but can be implemented on top of ArrayProperty + Gpu property with batched alloc, invalidate for reupload -> kinda complicated how this interact with playback, because we don't have a concept of ringbuffer playback)
        Sequence browsing (with optional prefetch / buffering):
        - Static data from disk (potentially variable size frames but can figure out a max size) (StreamingProperty + GpuProperty with streaming alloc)
        Spatial browsing (with optional prefetch / buffering):
        - Single frame of data (uncovered by our current system -> can be built as a custom renderable though. The property system is really only about time, not space)

    - Updates on Gpu Property variants:
        - Preuploaded:
            - update -> dynamic property with invalidate_frame() (uniform and mapped uses full LRU), (jagged or unmapped uses small LRU + gpu timeline copies)
            - append -> batched needs full recreate (and wait for idle or keep around until unused before destroying), non-batched can create and append a new buffer to list (requires renderer for bulk upload)
            - insert -> similar to above
            - remove -> similar to above but also need to keep in use buffer around until unused before destroying. (I prefer the idea of keeping around and manually calling destroy just to know when that will actually happen)
        - Streaming:
            - update -> invalidate_frame()  (just removes from LRU and increments generation index)
            - append -> works out of the box, frame index will not be found and will be uploaded next
            - insert -> requires invalidating all frame indices at and above insertion point (using new frame count) (can also just flush everything if faster)
            - remove -> requires invalidating all frame_index at and above remove point (using new frame count) (can also just flush everything if faster)

- Front-end (public property / primitive / user API):
    - [x] reuse Property and GpuProperty across different objects
        -> primitive (or user directly) specifies list of constraints when adding property and will aggregate (likely can do live aggregation don't need to store the actual list)
            - Buffer or Image flags -> just OR
            - Memory usage / pipeline stage -> ideally just pipeline stage and access and this will be used with lower level barrier API -> take earliest stage and or of accesses
            - Image layout -> use first specified or fallback to general if more than one given (potentially track this / warn for debugging perf issues)
            - mips will enable mips (if anyone wants them)
            - srgb will create an additional view of the image just for srgb purposes which is anyways
              what we most likely want. (in theory we could use default view only when possible
              but need a way to check if this is allowed).
        -> some usage/format combinations (especially on images) will not be allowed on some hardware,
           currently VMA / validation is catching this for us, but we might want to add a check for this
           at creation time to give a more useful combination. I think this is fine, the viewer will
           not reuse images on it's own so it should not run into this. If users do something that is
           not legal for their device it's better to know this explicitly than to provide an unexpected
           fallback to having multiple copies.
        -> Will not allow new users of a property that has already been
           realized, this will just error if you specify tighter constraints (can
           implement incrementally by adding constraint and checking if something
           changed), if needed users are expected to specify all constraints
           upfront and handle that (same rationale as above).
    - [x] free GpuProperty when dropped (do not hold references in renderer)
        -> hold gpu properties in property
        -> expose manual destroy to ensure instant destruction
    - [x] mip generation in materials
        -> set as a boolean on material, forwarded to ImageProperty
    - [x] material creation currently requires creating DataImageProperty manually,
          it makes sense from an implementation perspective because you need a way
          to distinguish between textures and arrays of scalars.
          Normal engines have a split at a higher level where you need to create
          textures before creating materials but we allow numpy arrays directly which
          maybe creates more problems than solutions.
          We have this tradeoff between userspace Buffer and Texture in normal engines
          vs userspace Property. Still not 100% sold on this.
    - [x] allow different dtypes for a property
        - Better definition of what you need to implement a property:
            All:
            - number of frames -> len()
            - dtype (to support multiple dtypes)
            - shape without frames (to support multiple shapes)
            - animation settings
            - upload settings
            Pre-allocated:
            - size of each element (size in bytes for buffer, width / height / format for image)
            Streaming:
            - size of max element  (size in bytes for buffer, width / height / format for image)
        - Things you want to verify on a property before using it
            - Shape constraints (potentially arbitrary rules)
            - dtype constraints (set of dtypes allowed)
        - Notes:
            - I think some of our issues come from the fact that we have the same constructor
              for users and for primitives that are just trying to typecheck a property.
            - User really just wants to do np.array -> Property and List[np.array] -> Property
                - Here there needs to be a balance between what is allowed
                - I'd prefer to not allow any transformation that allows copies, except
                  for scalars and tuples that kinda require it (but should be small anyways)
                  Not sure yet how to detect this case exactly though, maybe just a bunch of isinstance?
            - Primitive wants to know shape and dtype of property to make sure it's of the
              right shape for what it wants to do.
                - Here the plan is to ensure that a BufferProperty always exposes dtype shape
                  and primitives will first try to create one then decide what to do with it
        - More notes with found issues:
            - If you make a property out of an array you either need to know wanted dims or need to assume the first dimension is the frame (for a list you always assume it is, this does not cover list for tuple elements though, e.g. colors)
            - List-based data property has two use modes:
                - Jagged arrays of fixed length -> what we cover now
                - Appendable property (potentially jagged) -> more efficient way to do appends than np.append
            - Thoughts:
                * We don't fully cover the appenadable jagged property because we don't expose max_size (and you might want to append a bigger size at some point)
                * I think we leave the auto list -> BufferProperty in DataBufferProperty for the jagged array usecase and require people to do custom BufferProperty for the append to list with bigger size usecase
            - Auto-casting property shape (e.g. auto add anim dimension) is very convenient at caller side but requires
              shape constraint to be known at property add time (as we have now). However we have no way to specify
              complex constraints (could add tuple of shape, but what about arbitrary rules).
              We can also specify those variants as a set at primitive creation time but then we might require a lot of options.
              E.g. think about the case where a mesh could use many different dtypes and interleavings for attributes.
              Maybe inferring from dtype shape is not the play here and anyways we want a seperate API that exposes
              a better way to do custom attributes on a mesh. Like an AdvancedMesh or something.
            - Anyway we could at least enforce a BufferProperty to have a concrete dtype (similar to how an image needs a format)
              and leave special shape constraints to options on the primitive.
- Backend (non-public property and renderer API):
    - [x] multiple GpuProperties backed up by the same property?
        -> does not make sense, main use case would be to save CPU memory
           but you can create multiple properties over the same data without copying.
        -> somehow connected to multiple dtypes for a property, current behaviour is
           to astype() properties, but if you really care about memory you want to either
           create a view or error out if type is not ok. Maybe this should anyways be
           the default behavior (or opt in with some flag just for when you care).

Material notes:
- Channels:
    -> every material property is a single channel or a set of channels
    -> channels can be uniform values or textures
    -> if channels are on textures they could actually be in any channel and packed weirdly.
       I think we want max flexibility here. Can we have an API that makes the easy case easy but still allows full flexibilit?
    -> alpha could be optionally part of color / albedo or separate
    -> optional channels could also be useful, but how do we deal with permutations? can we have default values for some?
- How do we handle textures gpu upload, descriptors, shader permutations, uniform upload?
    - Gpu upload -> just gpu properties
    - Shader permutations -> we pre-compile all possibilities (primitives + materials if we have compatible fragment shader inputs, otherwise primitives x materials)
    - Descriptors and uniforms -> every renderable will have uniform buffer data and bind their material: make common interface / helpers for this
- How does this generalize to a path tracing shader? Try both raster and ray design together early -> plan is to reuse material object but serialize images into bindless descriptor set and collect data into storage buffers
- Raster shaders approach:
    - Rasterizer lighting is not really as streamlined as path tracer lighting where we can have a clear distinction between lights and materials.
      Things like environment light does not affect all materials equally.
    - I think for this design it makes more sense to have a full "shader" per material that does the light iteration and evaluation
    - We can make helper functions and try to share as much code as possible, but the high level flow will be in control of the "material" itself.
    - This can also be an opportunity for user to hook in and define their own fragment shaders / materials
- Path tracer approach:
    - The path tracer will still reuse material definitions, but will be a fully bindless design where materials for a frame will be uploaded before start of rendering.
- Bindings:
    - Two options. Full bindless (anyways needed for raytracing) vs per-material descriptor set
    - Full bindless:
        - Either use scene descriptor set (but need to find some other solution for lights) or some other one just for materials.
        - All textures will be 2D and sampled, so no need to manage that
        - All material definitions go into a buffer, similar to what we do with lights, we have to decide if better to do
          one buffer for each type or better to do a single buffer with the biggest size for all.
        - Only one buffer for all is maybe fine and is easier for ray-tracing, but limits a bit more complex materials in the
          future.
        - Can send material index / type as push constant or per-object uniform
    - Descriptor set:
        - One uniform binding with material data
        - Set of texture bindings with all textures needed for this material.
            - Either always specify the max number of textures and maybe don't set them
                3 options for unused descriptors:
                - use PARTIALLY_BOUND flag on descriptor binding to allow not bound if dynamically not used
                - use null descriptor -> requires roubstness2 and nullDescriptor (not available everywhere, e.g. on mac)
                - use dummy zero texture -> can substitute with null descritpor if available
            - Otherwise specify only the textures that are actually in use by this material
                -> more shader permutations, likely not worth it
- Shader permutations:
    - If we go the descriptor set way we then need to handle shader permutations based on which material type is going to be bound.
    - We anyways plan to have a cache for other things so that does not sound too bad
    - Thought we will need to pass through shader defines and handle them somehow.
        -> could also complicate a bit lookup for caching or precompilation because now shaders are also keyed by map of defines.

Light notes:
- Shadow plan:
    - Each light will have it's own shadow map for simplicity
    - Directional:
        - render each shadowmap in one pass
        - if multi-viewport is available we could batch directional lights with the same shadowmap size and render to a texture array
    - Pointlight and spotlight: (spotlight with angle < 90 can have single face, while > 90 will be the same as point light)
        - render each face in one pass
        - if multi-viewport is available we can render all faces in one pass
    - UniformEnv and Env will not have shadows (maybe AO at some point but that's separate)
    - Area light could be approximated by a point light, otherwise need raytraced shadows. In general would be cool to do LTC + ratio estimator here, but it's a big separate feature.
- Add options on lights to enable / disable shadows
- Add options on objects to disable casting shadows -> maybe later even allow keying lights

Dynamic light count notes:
- Uniform buffers might be faster but are very limited in terms of how dynamic we can make the light count.
- An explicit in memory representation could simplify this, but then also requires dynamic dispatch or separate / suballocated buffers
  and bindless resources / atlases for shadow maps. Atlases then require custom sampling and filtering.
- I don't think we want to do atlases, but we could do bindless. This would anyways be required for raytracing or large indirect
  dispatches.
- There is a world where we focus really on the bindless and can cache most
  object draws, materials and lights into indirect execution buffers and render a
  lot of objects really fast in python. However this comes at a cost of complexity
  in terms of tracking changes and manual invalidation. This trades ease of
  use for perf, but it's not currently obvious how worth the trade is.

Bindless:
- Extensions:
    - descriptor indexing is widely available but requires managing different pools per descriptor type
    - Descriptor buffer allows manually managing descriptor memory and copies
        - Nvidia: Kepler+
        - AMD: Old
        - Mac: not yet on MoltenVK, maybe soon?
        - Intel: Skylake+
    - Mutable descriptor is only available Turing+ and would allow single pool approach
        - Nvidia: Turing+
        - AMD: RX480+
        - Mac: not yet on MoltenVK, maybe soon?
        - Intel: Skylake+
- handles:
    - slang now has DescriptorHandle but it's 64 bit, this allows things like transparent combined image sampler with 2 indices,
      or casting from 64bit address to acceleration structure. But would not be needed for textures and buffers.
      With more manual work we can still implement bindless without it. Need to test this
- Plan:
    - Require 8 descriptor sets and descriptor indexing
    - Handle are 32 bit or 64 bit integers managed by application.
    - In shader we have utilities to convert from application indices to typed handles
    - We override dereference operator to implement our mapping from type to descriptor set, including things like generation/type checking
    - How do we do suballocation? Dynamic / static split?

- Resources:
    - Descriptor update vs content update
    - Content:
        - needs to be buffered per-frame if allocated in mapped memory and updated by the cpu, otherwise just uploaded once per frame
        - Frequencies:
            - once per frame -> lights and camera data
            - once per object -> transform
            - once per material -> material data
    - Descriptors:
        - needs to be buffered per-frame because interacted with from the CPU
        - Frequenceis:
            - once per frame -> shadow maps, lights and camera data if allocated in different buffers
            - once per object -> constant buffer descriptor
            - once per material -> material textures
    - Use push constant to pass indices into descriptor arrays and offsets into buffers
    -> Idea:
        - One descriptor set per type of resource make a bindless group, we have 1 bindless group per frame
        - once per frame data like scene constants are in fixed position (e.g. buffer 0)
        - once per object data is suballocated from a big buffer in fixed position (e.g. buffer 1), and offsets are passed in push constants
        - once per material data is also suballocated from a big buffer in fxed position
        - textures descriptors are written every time they change (e.g. material updates)

        - Create a material:
            - Alloc a gpu descriptor for each texture
            - Alloc some gpu memory into material pool buffer, write descriptor indices in them
        - Update a material:
            - Values -> just write to material buffer (frames in flight?)
            - Textures -> write new descriptor to descriptor buffer, material buffer uses the same index anyways
        - Free a material:
            - Release descriptors to pool
            - Release gpu memory to material buffer pool
    -> Thoughts:
        - If we want a flexible scheme like the one we have for constants (e.g. alloc from a buff and then expand with another buffer if full),
          we need to anyways have usage code get a buffer index to index the descriptor + a buffer offset to index within the buffer.
        - For buffers especially we could just switch to doing pointers then and just use 64bit addresses without worring about descriptors at all.
        - We still have to use descriptors for images and samplers, but that might simplify things a bit.

Rendering features:
    - Raster (normal viewer):
        - 2D and 3D viewports
        - multi-layer OIT (start from a PoC with depth peeling)
    - Path tracer:
        - Objects that could potentially be path traced:
            - Meshes
            - Other primitives -> should just be meshed or use intersection shaders? depends on complexity of primitive maybe?
            - Need light info too
        - Support for more complex camera models and effects
            - Lens distortion (could be useful even for visualizing different camera models)
            - DoF
            - Vignetting
        - Accumulation settings
        - Light sampling with alias tables
        - Raytracer design:
            - waverfront -> multi-pass, more complex
            - compute -> easiest
            - ray-tracing pipeline ->
        - Ray tracing pipeline with callable shaders
    - Plans:
        - Lights
            - Point, Directional and area first (maybe also leverage slang type, for path tracer)?
            - Environment -> only for path tracer or also do prefiltered IBL?
        - Materials
            - start with common material model for all objects, or maybe just a few materials with dynamic dispatch (leverage slang types)
        - Emissive:
            - Not sure if we need this or if better to just do mesh lights at first, we are not building a general purpose path tracer (or are we?)
        - Volumes (?)
- Ideas:
    - Should rendering code be per object or per-renderer?
        -> Per object has a clear path to user extensions, a bit more unclear for
        -> Per renderer can unlock more "batched" logic
            -> Can we still get the batch logic with a per-renderer approach?
    - Can we batch barriers for GPU uploads?
        - e.g.: instead of copy, barrier, copy, barrier, copy, barrier do copy, copy, copy, (barrier, barrier, barrier)
    - Generic viewer type with just basics + specialized viewer types for specific use cases (careful about composition vs specialization)
        - Scene viewer -> classic scene graph
            - Single frame viewer
            - Sequence viewer

Server:
-> 3 layers
    -> raw message (format, type, length, data)
    -> per format parser
    -> parsed message
    Protocols: tcp, http, websockets
    Formats: binary, json, msgpack, pickle (maybe behind off-by-default flag for better security?)
    Builtin-messages:
    - Frame / playback control
    - camera control
    - create, update, delete objects
-> what about REST? it might be convenient to speak with the viewer direclty in rest, different API?
   wrap this API into a rest API? e.g. JSON for body is same as this, and type encoded in endpoint? seems doable
    -> what we have now will be a TcpServer, can also have an  HttpServer and maybe others too?, basically different ways to produce a RawMessage
-> Handle shutdown of TcpServer
    -> exceptions in parsing raw messages should be handled gracefully and log (wrap async callback and print info)
    -> exceptions in main thread should still have the http server exit -> this does not seem to happen correctly atm (maybe connections are keeping this alive in read_exact?) need to switch to async?
-> try small http server and port of websockets server as PoC

Renderer:
-> think about what is the best way to support different types of rendering, and how to not duplicate a huge amount of code
-> 2D vs 3D, raster vs raytrace vs path trace (e.g. accumulation), quality mode (e.g. depth peeling, MSAA, etc..)
-> how does this play out with implicit prefetching / scene stepping? ideally orthogonal?

Thoughts:
-> Lets have the interface always be CPU objects, the distinction between data and streaming properties makes sense to
   me and allows users to customize how the data is loaded but still giving the easy interface with implicit conversion for arrays
-> Renderables should be able to constru
-> Later we can maybe provide some kind of escape hatch for giving gpu buffers directly for these properties, another option
   would be to have renderables that can
-> Big questions that remain:
    -> can properties be shared across objects? since animation is on the property, i dont see any issue with this
    -> If properties are shared, how to handle their GPU counterparts? Are those owned by objects?
       Are they part of the property itself but optional? How does the user configure prefetching vs preload?

- per frame uploadable property:
    -> per frame bar memory
    -> per frame cpu buffer backed by a single GPU buffer
        -> for some things this is similar to non-preuploaded resources
        -> similarities:
            - we already have CPU buffers or BAR buffers for each frame
        -> differences:
            - no need for async CPU load logic, data is provided every frame (or same as previous frame)
            - can be thought as keyed not by animation frame but by a monotonically increasing global frame counter
            - prefetching does not make sense because we cannot predict future
        Questions:
        -> can we create a sort of LiveProperty that implements this?
        -> or should we instead build the non-preuploaded resources on top of something that supports this?
    -> I think we are in for a big rewrite here:
        [x] we don't actually need more than 1 GPU buffer unless prefetching:
            -> BAR + CPU is anyways on CPU buffers that are per-frame buffered (currently BAR is on GPU buffer but maybe should be moved)
            -> GFX + TRANSFER non-prefetch buffers do not need double buffering because we are not overlapping frames like that
            -> we can keep treating prefetch buffers separate how we have been doing
        -> live data is not keyed to a specific frame. If we have invalidation we can have properties with 1 frame (always at frame 0) and update it.
        -> we actually want a way to invalidate CPU and GPU buffers if the data has changed.
            -> this will trigger a new upload using the same mechanisms and will cover the
        -> for non-streaming upload, do we lazily alloc upload cpu buffers, can we still opt in when creating custom properties?
            -> also think about easy way to customize upload settings, e.g. if we don't yet know the wanted dtype shape
            -> somehow related is also that maybe for some properties multiple dtypes are allowed, and sometimes even shapes:
                - textures
                - vertex attribute dtypes
                - vertex attribute number of joint indices / weights
                -> do we have a cleaner way to handle those other than (-1, -1, -1) as we do with images now?

        Design space:
            CPU property memory allocation:
                Full array   -> one single array with all frames of the same size
                Jagged array -> list of variable size arrays

            GPU property variants:
                Preupload: All frames allocated with max size (conservative preupload) or exact size (exact preupload)
                    Buffer location:
                        CPU mapped  -> one for each property frame with exact or conservative size depending CPU property, upload is just memcpy
                        GPU managed -> one for each property frame with exact or conservative size depending CPU property, upload through bulk
                    CPU load:
                        sync  -> just use at upload time
                        async -> parallel for at creation time
                    GPU upload (only if GPU managed): bulk upload
                    Prefetching: N/A
                    Update:
                        CPU mapped:
                            Full array
                                1. can alloc extra staging buffers and exchange them with frames in flight -> similar to gpu managed
                            Jagged array -> interchange is not possible because we can get back smaller buffers, there is no perfect solution for this case I think.
                                1. Switch to always have the GPU do the copy (basically promote to GPU managed and handle the same way) -> potentially bad for resources in CPU memory
                                2. Allocate frames_in_flight - 1 buffers FOR EACH property frame -> adds frames_in_flight - 1 times the cpu memory usage of this property
                                3. Add an externally synchronized CPU thread to do the async copy when the last frame is done but before the new frame uses the buffer.
                                This basically just replaces the GPU copy with a CPU copy on the same timeline and critical path. Synchronization will also be very hard to get right.
                                I think we should do solution 1 by default, and expose solution 2 if explicitly requested by the user.
                        GPU managed: can allocate staging buffers for each frame in flight with conservative size and issue uploads each render frame
                    Resize:
                        Shrink:
                            Full array: need to reallocate all buffers anyways, might aswell create a new property, use a jagged array if needed
                            Jagged array: supported out of the box, GPU buffers are not resized, uploads and rendering checks actual size before usage
                        Grow:
                            Full array: need to reallocate all buffers anyways, might aswell create a new property
                            Jagged array: could potentially reallocate the buffer just for this frame, but need a way to delay freeing of existing buffer until not in use (could always keep track, or just wait a safe amount of frames)

                Streaming: Frames in flight allocated with max size  -> upload every miss (e.g. new property frame not yet loaded, allows prefetching)
                    Buffer location:
                        CPU mapped  -> need one per frame in flight, don't reuse until render frame finished on the GPU
                        GPU managed -> need one cpu buffer as a source per frame in flight, one GPU buffer is enough because GPU frames don't overlap
                    CPU load:
                        sync  -> CPU property frame loaded immediately on .load()
                        async -> CPU proeprty frame enqueued on .load() and waited on .upload() (wait could potentially be delayed until submit for GPU copy, and also for CPU if scheduled asynchronously)
                    GPU upload (only if GPU managed):
                        gfx queue      -> extra commands in gfx queue to issue load
                        transfer queue -> extra commands in transfer queue to issue load + extra semaphores to sync queues
                    Prefetching:
                        CPU prefething  -> only if CPU load is async, has extra buffers for load jobs
                        GPU prefetching -> only if GPU load is transfer queue, has extra buffers and command lists for async jobs, requires extra semaphores to sync queues
                    Update:
                        Same as streaming in a new property frame, in a way the frame identifier becomes a tuple (property frame index, generation index)
                        The tricky bit is that we really need to consider both at the same time, because you could potentially in flight
                        the same property frame but with a different generation index.
                    Resize:
                        Shrink: supported out of the box, GPU buffers are not resized, uploads and rendering checks actual size before usage
                        Grow: need to reallocate all buffers anyways, might aswell create a new property

                All variants:
                    - preupload | preuplod dynamic | streaming
                    - cpu sync | cpu async
                    - cpu heap | gpu heap
                    - cpu mapped | gfx queue | transfer queue
                    - cpu prefetch | no prefetch
                    - gpu prefetch | no prefetch

                    Hard rules:
                        gpu prefetch requires transfer queue
                        cpu prefetch requires cpu async
                        transfer queue requires streaming
                        images require gpu heap
                    Soft rules:
                        gfx queue and transfer queue prefer gpu heap
                        preupload dynamic prefers gpu heap

                Data structures:
                    - Single buffer -> good for streaming content that is synchronized on GPU timeline
                    - Full array (for each property frame)  -> good for static preupload and reuse
                    - Ringbuffer (for each frame in flight) -> good for dynamic things that change each frame
                    - LRU cache -> good for prefetching and reuse of already uploaded buffers with eviction policies
                        -> handles frames in flight refcounting
                        -> handles LRU eviction
                        -> handles lookup of already present property frames

                What datastructures we need for what usecase and set of buffers?
                    Preupload:
                        static: one buffer for each property frame -> Full array
                        dynamic:
                            Full array:
                                Cpu mapped: LRU pool of all cpu buffers -> tricky because we don't want LRU eviction but always want to evict the previous version of a frame
                            Jagged array:
                                case 1: - one buffer for each property frame and one staging buffer for each frame in flight -> Full array + Ring buffer
                                case 2: frames in flight buffers for each property frame -> Full array of ring buffers
                    Streaming:
                        CPU load: one buffer per frame in flight and caching -> LRU pool
                        GPU upload: single buffer

                    Streaming + prefetch:
                        CPU load: one buffer per frame in flight + one per prefetch and caching -> LRU pool
                        GPU upload: single buffer + prefetch buffers with exchange -> LRU pool

                Fields in use:
                    - resources -> preupload static mapped and unmapped
                    - cpu_pool -> preupload dynamic mapped and unmapped
                    -

                Preferred upload modes:
                    Integrated:
                        - If using an integrated GPU we want to avoid extra copies, therefore both CPU and BAR (Device mapped) should always be preferred (BAR because some integrated gpus could have "faster" memory).
                          We will prioritize doing uploads on the CPU directly, but can also let the GPU do BAR copies in the jagged dynamic case.
                    Discrete:
                        - Initial uploads could use DEVICE_MAPPED_WITH_FALLBACK, check if mapped and do initial upload with BAR.
                        - We could have a size heuristic if stuff could also stay in BAR later, while for bigger resources we want TRANSFER or GFX.

                Questions:
                [x] Subclass Property into ImageProperty and BufferProperty? Do I also need DataImageProperty and DataBufferProperty though? maybe not a good idea
                [x] How do we detect jagged vs not, DataProperty knows this, but not BufferProperty? can do this at preupload time
                [x] How do we implement static -> dynamic promotion in a way that does not require tons of if statements? we bite the bullet and do it for now
                [x] What is the user API for frame invalidation? Is the user expected to call the method on the gpu property? If so then likely have to document this well or be strict with naming? Currently yes
                [x] Which LRU pools are invalid, and in which states? Do we want to split some of this logic into components instead of having a bunch of state in GpuResourceProperty? we just reuse them for now
                [x] Simplify if statements by precomputing commonly used bools
                [x] Where do we implement and how do we override / configure upload mode picking (see preferred upload modes above)? We have an ok version of this in renderer (unless overridden by config)
                [x] Double check if we need the extra array of resource generations for the full array dynamic case
                [ ] How to handle append, insert, remove -> also need to remove gpu properties from renderer
                [ ] Improve UI of GPU property depending on various states

User texture for UI stuff, e.g. in imgui window (or tooltip):
    - ImGui uses VkDescriptorSet, must be a single COMBINED_IMAGE_SAMPLER in it
    - You can pass the descriptor pool from which to allocate to imgui directly, but we currently
      let ImGui do that and just pass a number
    - You can allocate from ImGui's pool with ImGuiImplVulkan_AddTexture and ImGuiImplVulkan_RemoveTexture,
      these will allocate from the imgui's managed pool. This only requires a VkSampler, an image view and a layout.
    - We can also allocate those descriptor sets manually, either directly in python or in a Gui wrapper.
    - It might be a bit inefficient to have a DescriptorSet and Pool per image, but we currently don't expose
      descriptor pools anyway. We might want to revisit this once we do this.
